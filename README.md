# Machine Learning Research Repository

This repository contains various machine learning and neural network implementations, focusing on transformer architectures, quantum machine learning, and reinforcement learning approaches.

## Repository Contents

### Jupyter Notebooks

- **`Autoformer_Jan2014.ipynb`** - Implementation of Autoformer architecture
- **`Autoformer_enhanced.ipynb`** - Enhanced version of the Autoformer model
- **`MSMVAN_Jan2014.ipynb`** - Multi-Scale Multi-View Attention Network implementation
- **`Msvan_RL_Combo.ipynb`** - Combination of MSMVAN with Reinforcement Learning

### Python Scripts

- **`QlstmCode`** - Quantum LSTM implementation using PennyLane and TensorFlow
- **`msvan_rl_transformer`** - Reinforcement Learning with Transformer architecture using PyTorch

## Quick Start

### Running Jupyter Notebooks

The notebooks are configured to run in Google Colab. Click on the "Open in Colab" badge at the top of each notebook to run them directly in your browser.

### Running Python Scripts

```bash
# For Quantum LSTM
python QlstmCode

# For RL Transformer
python msvan_rl_transformer
```

## Dependencies

### Main Libraries
- **TensorFlow** - For neural network implementations
- **PyTorch** - For deep learning models
- **PennyLane** - For quantum machine learning
- **Stable Baselines3** - For reinforcement learning
- **NumPy** - For numerical computations
- **Pandas** - For data manipulation
- **Matplotlib** - For visualization
- **Scikit-learn** - For machine learning utilities
- **Gymnasium** - For RL environments

### Installation

```bash
pip install tensorflow torch pennylane stable-baselines3 numpy pandas matplotlib scikit-learn gymnasium
```

## Usage

1. **For Notebooks**: Open in Google Colab using the provided badges or run locally with Jupyter
2. **For Scripts**: Execute directly with Python after installing dependencies
3. **For Research**: Each implementation can be used as a starting point for further research and experimentation

## Features

- **Transformer Architectures**: Multiple transformer implementations including Autoformer
- **Quantum Machine Learning**: QLSTM combining quantum computing with neural networks
- **Reinforcement Learning**: RL-enhanced models with transformer architectures
- **Multi-Scale Attention**: Advanced attention mechanisms for improved model performance

## Contributing

Feel free to fork this repository and submit pull requests for improvements or new features.

## License

Please check individual files for specific licensing information.
